{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DER calculation\n",
    "This file is used to calculate Diarization Error Rate. It is calculated as follows:\n",
    "1. For each speaker, we calculate the number of false alarms, missed detections and overlapped speech.\n",
    "2. We sum up the number of false alarms, missed detections and overlapped speech.\n",
    "3. We divide the sum by the total number of speaker speech segments.\n",
    "4. The result is the DER.\n",
    "\n",
    "The formula is as follows:\n",
    "$$\n",
    "DER = \\frac{FA + MISS + OVER}{N_{spk}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $FA$ is the number of false alarms\n",
    "- $MISS$ is the number of missed detections\n",
    "- $OVER$ is the number of overlapped speech\n",
    "- $N_{spk}$ is the total number of speaker speech segments\n",
    "\n",
    "The code below is used to calculate the DER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyannote.core import Annotation, Segment\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Paths to the different RTTM files\n",
    "RTTM_TRUTH = \"../Dataset/RTTMs/Test\"\n",
    "\n",
    "RTTM_ORACLE_VAD = \"../Results/Oracle_vad/Test\"\n",
    "RTTM_ORACLE_DECODER = \"../Results/Oracle_decoder/Test\"\n",
    "RTTM_PYANNOTE = \"../Results/Ptannote/Test\"\n",
    "\n",
    "\n",
    "RESULTS_DIRECTORY = \"../Results/DER Results\"\n",
    "\n",
    "\n",
    "# Load the RTTM files\n",
    "def load_rttm(filename):\n",
    "    \"\"\"Load RTTM file and convert it to pyannote.core.Annotation\"\"\"\n",
    "    annotation = Annotation()\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "            file_id, channel, start_time, duration, _, _, *speaker_parts = parts\n",
    "            speaker = \" \".join(speaker_parts)\n",
    "            start_time = float(start_time)\n",
    "            duration = float(duration)\n",
    "            end_time = start_time + duration\n",
    "            segment = Segment(start_time, end_time)\n",
    "            annotation[segment] = speaker\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def calculate_der_metrics(\n",
    "    ground_truth_path, prediction_path, output_path, output_csv=\"der_results.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    This function calculates DER metrics for diarization and saves them to a CSV file,\n",
    "    including a row with average metrics.\n",
    "\n",
    "    Args:\n",
    "        ground_truth_path: Path to the directory containing ground truth RTTM files.\n",
    "        prediction_path: Path to the directory containing prediction RTTM files.\n",
    "        output_path: Path (including filename) for the output CSV file. Defaults to \"der_results.csv\" if not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the diarization error rate metric\n",
    "    der_metric = DiarizationErrorRate()\n",
    "\n",
    "    # Store results\n",
    "    results = []\n",
    "    total_der = 0\n",
    "    total_miss_duration = 0\n",
    "    total_speaker_error_duration = 0\n",
    "    total_false_alarm_duration = 0\n",
    "    total_duration = 0\n",
    "\n",
    "    # Iterate over files in prediction directory\n",
    "    for file_name in tqdm(os.listdir(prediction_path)):\n",
    "        truth_file = os.path.join(ground_truth_path, file_name)\n",
    "        prediction_file = os.path.join(prediction_path, file_name)\n",
    "\n",
    "        if os.path.exists(truth_file) and os.path.exists(prediction_file):\n",
    "            ground_truth = load_rttm(truth_file)\n",
    "            prediction = load_rttm(prediction_file)\n",
    "\n",
    "            # Calculate DER and components manually\n",
    "            detailed = der_metric(ground_truth, prediction, detailed=True)\n",
    "            #print(detailed)\n",
    "\n",
    "            der = detailed[\"diarization error rate\"]\n",
    "            confusion_duration = detailed[\"confusion\"]\n",
    "            false_alarm = detailed[\"false alarm\"]\n",
    "            missed_speech = confusion_duration - false_alarm\n",
    "            total_file_duration = detailed[\"total\"]\n",
    "\n",
    "            # Accumulate totals for average calculation\n",
    "            total_der += der\n",
    "            total_miss_duration += missed_speech\n",
    "            total_speaker_error_duration += confusion_duration - missed_speech\n",
    "            total_false_alarm_duration += false_alarm\n",
    "            total_duration += total_file_duration\n",
    "\n",
    "            # Append results for this file\n",
    "            results.append(\n",
    "                {\n",
    "                    \"File\": file_name,\n",
    "                    \"DER\": der,\n",
    "                    \"Miss Duration\": missed_speech / total_file_duration,\n",
    "                    \"Speaker Error Duration\": confusion_duration / total_file_duration\n",
    "                    - missed_speech / total_file_duration,\n",
    "                    \"False Alarm Duration\": false_alarm / total_file_duration,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(f\"File {file_name} not found\")\n",
    "            sys.exit(1)  # Exit with an error if a file is missing\n",
    "\n",
    "    # Calculate average metrics (assuming all files have processed)\n",
    "    if total_duration > 0:  # Avoid division by zero\n",
    "        average_der = total_der / len(results)\n",
    "        average_miss_duration = total_miss_duration / total_duration\n",
    "        average_speaker_error_duration = total_speaker_error_duration / total_duration\n",
    "        average_false_alarm_duration = total_false_alarm_duration / total_duration\n",
    "\n",
    "        # Create a dictionary for average metrics\n",
    "        average_metrics = {\n",
    "            \"File\": \"Average\",\n",
    "            \"DER\": average_der,\n",
    "            \"Miss Duration\": average_miss_duration,\n",
    "            \"Speaker Error Duration\": average_speaker_error_duration,\n",
    "            \"False Alarm Duration\": average_false_alarm_duration,\n",
    "        }\n",
    "\n",
    "        # Append average metrics to results\n",
    "        results.append(average_metrics)\n",
    "\n",
    "    # Convert results to DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Convert results to DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/232 [00:00<?, ?it/s]c:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\pyannote\\metrics\\utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 56/232 [00:29<00:46,  3.79it/s]"
     ]
    }
   ],
   "source": [
    "# Calculate DER metrics for Oracle VAD\n",
    "calculate_der_metrics(\n",
    "    RTTM_TRUTH, RTTM_ORACLE_VAD, os.path.join(RESULTS_DIRECTORY, \"der_oracle_vad.csv\")\n",
    ")\n",
    "\n",
    "# Calculate DER metrics for Oracle Decoder\n",
    "calculate_der_metrics(\n",
    "    RTTM_TRUTH, RTTM_ORACLE_DECODER, os.path.join(RESULTS_DIRECTORY, \"der_oracle_decoder.csv\")\n",
    ")\n",
    "\n",
    "# Calculate DER metrics for \n",
    "calculate_der_metrics(\n",
    "    RTTM_TRUTH, RTTM_PYANNOTE, os.path.join(RESULTS_DIRECTORY, \"der_pyannote.csv\")\n",
    ")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
