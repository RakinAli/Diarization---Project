{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom clustering\n",
    "In this code we attempt to do custom clustering on the data. We will use the following steps:\n",
    "- Preprocessing the data    \n",
    "    - Get melspectogram of audio with 80 features per mel-frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_vad import (\n",
    "    get_speech_timestamps,\n",
    ")  # Assuming this file defines the get_speech_timestamps function\n",
    "from preprocess import Wav2Mel\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables used for preprocessing\n",
    "SAMPLE_RATE = 16000\n",
    "NORM_DB = -3\n",
    "FFT_WINDOW_MS = 25\n",
    "FFT_HOP_MS = 10\n",
    "\n",
    "BLOCK_SIZE = 50  # MFCC frames to stack together for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "\n",
    "# Pre-trained Dvector model --> Embedding model (modify paths as needed)\n",
    "dvector_model = torch.jit.load(\"Pretrained Modules/dvector-step250000.pt\")\n",
    "\n",
    "# V Pretrained VAD model --> VAD model (modify paths as needed)\n",
    "vad_model = torch.jit.load(\"Pretrained Modules/silero_vad.jit\")\n",
    "\n",
    "wave2mel = Wav2Mel(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    norm_db=NORM_DB,\n",
    "    fft_window_ms=FFT_WINDOW_MS,\n",
    "    fft_hop_ms=FFT_HOP_MS,\n",
    "    n_mels=40,\n",
    ")\n",
    "\n",
    "\n",
    "# In the case that you wish to verify the model architecture\n",
    "#! dvector_model.eval()\n",
    "#! vad_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "sox extension is not supported on Windows",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Preprocess the audio\u001b[39;00m\n\u001b[0;32m     59\u001b[0m wave_tensor, sr \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(audio_path)\n\u001b[1;32m---> 60\u001b[0m wave_tensor, mel_tensor \u001b[38;5;241m=\u001b[39m wave2mel(wave_tensor, sr)\n\u001b[0;32m     63\u001b[0m audio_tensor \u001b[38;5;241m=\u001b[39m preprocess_audio(audio_path)\n\u001b[0;32m     64\u001b[0m audio_mel_tensor \u001b[38;5;241m=\u001b[39m compute_mel_spectrogram(audio_tensor, SAMPLE_RATE)\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rakin\\Documents\\GitHub\\Speaker-Diarization\\Main code\\preprocess.py:36\u001b[0m, in \u001b[0;36mWav2Mel.forward\u001b[1;34m(self, wav_tensor, sample_rate)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, wav_tensor: torch\u001b[38;5;241m.\u001b[39mTensor, sample_rate: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 36\u001b[0m     wav_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msox_effects(wav_tensor, sample_rate)\n\u001b[0;32m     37\u001b[0m     mel_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_melspectrogram(wav_tensor)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wav_tensor, mel_tensor\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rakin\\Documents\\GitHub\\Speaker-Diarization\\Main code\\preprocess.py:53\u001b[0m, in \u001b[0;36mSoxEffects.forward\u001b[1;34m(self, wav_tensor, sample_rate)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, wav_tensor: torch\u001b[38;5;241m.\u001b[39mTensor, sample_rate: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 53\u001b[0m     wav_tensor, _ \u001b[38;5;241m=\u001b[39m apply_effects_tensor(wav_tensor, sample_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffects)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wav_tensor\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torchaudio\\sox_effects\\sox_effects.py:156\u001b[0m, in \u001b[0;36mapply_effects_tensor\u001b[1;34m(tensor, sample_rate, effects, channels_first)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_effects_tensor\u001b[39m(\n\u001b[0;32m     56\u001b[0m     tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     57\u001b[0m     sample_rate: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m     58\u001b[0m     effects: List[List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m     59\u001b[0m     channels_first: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply sox effects to given Tensor\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    .. devices:: CPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m        >>> assert sample_rate == 8000\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sox_ext\u001b[38;5;241m.\u001b[39mapply_effects_tensor(tensor, sample_rate, effects, channels_first)\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torchaudio\\_extension\\utils.py:121\u001b[0m, in \u001b[0;36m_LazyImporter.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_import_once()\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, item)\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torchaudio\\_extension\\utils.py:135\u001b[0m, in \u001b[0;36m_LazyImporter._import_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_import_once\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimport_func()\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;66;03m# By attaching the module attributes to self,\u001b[39;00m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;66;03m# module attributes are directly accessible.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;66;03m# This allows to avoid calling __getattr__ for every attribute access.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torchaudio\\_extension\\utils.py:85\u001b[0m, in \u001b[0;36m_init_sox\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_sox\u001b[39m():\n\u001b[1;32m---> 85\u001b[0m     ext \u001b[38;5;241m=\u001b[39m _import_sox_ext()\n\u001b[0;32m     86\u001b[0m     ext\u001b[38;5;241m.\u001b[39mset_verbosity(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01matexit\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rakin\\anaconda3\\Lib\\site-packages\\torchaudio\\_extension\\utils.py:66\u001b[0m, in \u001b[0;36m_import_sox_ext\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_import_sox_ext\u001b[39m():\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msox extension is not supported on Windows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m eval_env(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCHAUDIO_USE_SOX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msox extension is disabled. (TORCHAUDIO_USE_SOX=0)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: sox extension is not supported on Windows"
     ]
    }
   ],
   "source": [
    "# Function to preprocess audio (resample and normalize)\n",
    "def preprocess_audio(audio_path, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Preprocesses audio using Librosa. Includes resampling and normalization (default -3 dB).\n",
    "\n",
    "    Args:\n",
    "        audio_path: Path to the audio file.\n",
    "        sample_rate: Target sample rate (default 16kHz).\n",
    "\n",
    "    Returns:\n",
    "        audio_tensor: Resampled and normalized audio tensor (channel dimension added).\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=None)  # Load with original sample rate\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sr != sample_rate:\n",
    "        y = librosa.effects.resample(y, sr, sample_rate)\n",
    "\n",
    "    # Normalize to -3 dB (default)\n",
    "    y = librosa.util.normalize(y)  # Removed norm_db argument\n",
    "\n",
    "    # Convert to torch tensor with channel dimension\n",
    "    audio_tensor = torch.from_numpy(y).unsqueeze(0)\n",
    "\n",
    "    return audio_tensor\n",
    "\n",
    "\n",
    "# Function to compute Mel spectrogram using Librosa\n",
    "def compute_mel_spectrogram(audio_tensor, sample_rate):\n",
    "    \"\"\"\n",
    "    Computes Mel spectrogram from preprocessed audio tensor using Librosa.\n",
    "\n",
    "    Args:\n",
    "        audio_tensor: Preprocessed audio tensor (with channel dimension).\n",
    "        sample_rate: Sample rate of the audio.\n",
    "\n",
    "    Returns:\n",
    "        mel_tensor: Mel spectrogram tensor (float).\n",
    "    \"\"\"\n",
    "    audio = audio_tensor.squeeze(0).numpy()  # Remove channel dimension\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate) # Default n_fft=2048, hop_length=512\n",
    "    mel_tensor = torch.from_numpy(melspectrogram).float() # Convert to tensor\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def get_frames(mel_tensor, block_size):\n",
    "    return mel_tensor.unfold(0, block_size, block_size).mT\n",
    "\n",
    "def get_frame_embeddings(mel_frames):\n",
    "    embeddings = torch.empty(mel_frames.shape[0], 256)\n",
    "    for frame_idx in range(mel_frames.shape[0]):\n",
    "        embeddings[frame_idx, :] = dvector_model.embed_utterance(mel_frames[frame_idx])\n",
    "    return embeddings.detach().numpy()\n",
    "\n",
    "\n",
    "# Testing for one audio file\n",
    "audio_path = \"../Dataset/Audio/Dev/afjiv.wav\"\n",
    "audio_tensor = preprocess_audio(audio_path)\n",
    "audio_mel_tensor = compute_mel_spectrogram(audio_tensor, SAMPLE_RATE)\n",
    "\n",
    "# analyze the audio\n",
    "speech_timestamps = get_speech_timestamps(\n",
    "    audio_tensor, vad_model, sampling_rate=SAMPLE_RATE, return_seconds=True\n",
    ")\n",
    "\n",
    "\n",
    "# Get the mel frames\n",
    "mel_frames = get_frames(audio_mel_tensor, BLOCK_SIZE)\n",
    "print(\"Mel_frame dims\", mel_frames.shape)\n",
    "frame_embeddings = get_frame_embeddings(mel_frames)\n",
    "\n",
    "# Cluster the embeddings\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(frame_embeddings)\n",
    "print(kmeans.labels_)\n",
    "print(speech_timestamps)\n",
    "print(kmeans.labels_)\n",
    "print(len(kmeans.labels_))\n",
    "print(len(speech_timestamps))\n",
    "plt.plot(kmeans.labels_)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
