{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "In this part of the notebook, we will preprocess the data to make it ready for training. We will do the following steps:\n",
    "1. Visualize the RTM data\n",
    "2. Check the sampling rate of the data and resample to consist to 16kHz to enssure uniformity across the dataset\n",
    "3. Extract MFCC features from the audio data and allign with the RTM data to create the final dataset\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the RTM data\n",
    "\n",
    "# All importss\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as librosa\n",
    "import sys\n",
    "\n",
    "\n",
    "# Parse the dummy RTTM file\n",
    "def parse_rttm(file_path):\n",
    "    columns = [\n",
    "        \"Type\",\n",
    "        \"File ID\",\n",
    "        \"Channel ID\",\n",
    "        \"Turn Onset\",\n",
    "        \"Turn Duration\",\n",
    "        \"Orthography Field\",\n",
    "        \"Speaker Type\",\n",
    "        \"Speaker Name\",\n",
    "        \"Confidence Score\",\n",
    "        \"Signal Lookahead Time\",\n",
    "    ]\n",
    "    df = pd.read_csv(file_path, sep=\"\\s+\", names=columns)\n",
    "    return df[[\"Turn Onset\", \"Turn Duration\", \"Speaker Name\"]]\n",
    "\n",
    "\n",
    "# Grabs the important data from the RTM file and creats End Time column\n",
    "def prepare_data(rttm_data):\n",
    "    rttm_data[\"Turn Onset\"] = rttm_data[\"Turn Onset\"].astype(float)\n",
    "    rttm_data[\"Turn Duration\"] = rttm_data[\"Turn Duration\"].astype(float)\n",
    "    rttm_data[\"End Time\"] = rttm_data[\"Turn Onset\"] + rttm_data[\"Turn Duration\"]\n",
    "    return rttm_data\n",
    "\n",
    "\n",
    "# Plots the timeline of the speakers\n",
    "def plot_timeline(data):\n",
    "    \"\"\"\n",
    "    Plots the speaker timeline based on the provided data.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The data containing speaker information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    speakers = data[\"Speaker Name\"].unique()\n",
    "    speaker_indices = {speaker: idx for idx, speaker in enumerate(speakers)}\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        start = row[\"Turn Onset\"]\n",
    "        end = row[\"End Time\"]\n",
    "        speaker = row[\"Speaker Name\"]\n",
    "        ax.plot([start, end], [speaker_indices[speaker]] * 2, linewidth=10)\n",
    "\n",
    "    ax.set_yticks(range(len(speakers)))\n",
    "    ax.set_yticklabels(speakers)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Speakers\")\n",
    "    ax.set_title(\"Speaker Timeline\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def debugger_test():\n",
    "    # Test the RTM data\n",
    "    rttm_data = parse_rttm(\"../Dataset/RTMS/Dev/abjxc.rttm\")\n",
    "    rttm_data = prepare_data(rttm_data)\n",
    "    plot_timeline(rttm_data)\n",
    "\n",
    "\n",
    "def debugger_run_all():\n",
    "    devRTM_path = \"../Dataset/RTMS/Dev/\"\n",
    "    # Visaualize all RTM data in the folder devRTM_path\n",
    "\n",
    "    # Get all the files in the folder\n",
    "    files = os.listdir(devRTM_path)\n",
    "    for file in files:\n",
    "        if file.endswith(\".rttm\"):\n",
    "            file_path = os.path.join(devRTM_path, file)\n",
    "            rttm_data = parse_rttm(file_path)\n",
    "            rttm_data = prepare_data(rttm_data)\n",
    "            plot_timeline(rttm_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_rate, \n"
     ]
    }
   ],
   "source": [
    "# Get sampling rate of all audio files, allign mfcc with RTM data such that they can be used for training\n",
    "\n",
    "# Get the MFCC of the audio file using librosa\n",
    "def get_mfcc(file_path):\n",
    "    \"\"\"\n",
    "    Compute the Mel-frequency cepstral coefficients (MFCC) for an audio file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "    mfcc (ndarray): The computed MFCC coefficients.\n",
    "    sr (int): The sample rate of the audio file.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Use 13 MFCC coefficients\n",
    "    return mfcc, sr\n",
    "\n",
    "\n",
    "# Align the MFCC data with the RTM data -- Speaker segmentation for training\n",
    "def align_mfcc(mfcc_data, sr, rttm_path, hop_length=220):\n",
    "    \"\"\"\n",
    "    Aligns MFCC data with RTTM data using a specified hop length.\n",
    "\n",
    "    Args:\n",
    "        mfcc_data (numpy.ndarray): MFCC data.\n",
    "        sr (int): Sampling rate.\n",
    "        rttm_path (str): Path to the RTTM file.\n",
    "        hop_length (int, optional): Number of samples between successive frames. Defaults to 220.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: Each dictionary contains 'Speaker Name' and 'MFCC Segment'.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If required columns are missing in the DataFrame.\n",
    "    \"\"\"\n",
    "    rttm_data = parse_rttm(rttm_path)\n",
    "    rttm_data = prepare_data(rttm_data)  # Ensure this doesn't drop necessary columns\n",
    "\n",
    "    # Check if necessary columns exist\n",
    "    if \"End Time\" not in rttm_data.columns or \"Turn Onset\" not in rttm_data.columns:\n",
    "        raise KeyError(\"Necessary columns are missing from RTTM data.\")\n",
    "\n",
    "    # Convert RTTM times to frame indices\n",
    "    rttm_data[\"Start Frame\"] = (rttm_data[\"Turn Onset\"] * sr / hop_length).astype(int)\n",
    "    rttm_data[\"End Frame\"] = (rttm_data[\"End Time\"] * sr / hop_length).astype(int)\n",
    "\n",
    "    segments = []\n",
    "    num_frames = mfcc_data.shape[1]\n",
    "\n",
    "    for _, row in rttm_data.iterrows():\n",
    "        start_frame = row[\"Start Frame\"]\n",
    "        end_frame = row[\"End Frame\"]\n",
    "\n",
    "        # Ensure start_frame and end_frame are within the bounds of MFCC data length\n",
    "        if start_frame >= num_frames:\n",
    "            continue  # Skip segments that start beyond the audio length\n",
    "        if end_frame > num_frames:\n",
    "            end_frame = num_frames\n",
    "\n",
    "        segment_mfcc = mfcc_data[:, start_frame:end_frame]\n",
    "        segments.append(\n",
    "            {\"Speaker Name\": row[\"Speaker Name\"], \"MFCC Segment\": segment_mfcc}\n",
    "        )\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Visualize the MFCC data with speaker labels --> Useless for now\n",
    "def visualize_MFCC(mfcc_data, speaker_labels, sr, hop_length=512, duration=1.0):\n",
    "    \"\"\"\n",
    "    Visualize the MFCC data with speaker labels.\n",
    "\n",
    "    Args:\n",
    "        mfcc_data (numpy.ndarray): MFCC data.\n",
    "        speaker_labels (list): Speaker labels for each MFCC frame.\n",
    "        sr (int): Sample rate of the audio file.\n",
    "        hop_length (int, optional): Hop length used in the MFCC calculation. Defaults to 512.\n",
    "        duration (float, optional): Duration in seconds to display. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Calculate the number of frames to display for the given duration\n",
    "    num_frames = int((duration * sr) / hop_length)\n",
    "\n",
    "    # Limit the mfcc_data and speaker_labels to the specified duration\n",
    "    mfcc_data = mfcc_data[:, :num_frames]\n",
    "    speaker_labels = speaker_labels[:num_frames]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    for idx, (mfcc_frame, speaker_label) in enumerate(zip(mfcc_data.T, speaker_labels)):\n",
    "        ax.plot(mfcc_frame + idx * 20, label=speaker_label)\n",
    "\n",
    "    ax.set_xlabel(\"MFCC Coefficients\")\n",
    "    ax.set_ylabel(\"Frame Index\")\n",
    "    ax.set_title(\"MFCC Visualization\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Test get_mfcc on filepath\n",
    "path_wave = \"../Dataset/Audio/Dev/afjiv.wav\"\n",
    "path_rttm = \"../Dataset/RTMS/Dev/afjiv.rttm\"\n",
    "\n",
    "# Get the MFCC and allign it with the RTM data for training --> Working\n",
    "mfcc_test, sampling_rate = get_mfcc(path_wave)\n",
    "test_rttm_data = parse_rttm(path_rttm)\n",
    "test_rttm_data = prepare_data(test_rttm_data)\n",
    "test_alligned = align_mfcc(mfcc_test,sampling_rate,path_rttm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the entire dataset for training and testing\n",
    "Here we will go through all audio/dev wave files and extract the MFCC features and allign with the RTM data to create the final dataset for training and validation. Pick validation of 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes to the \n",
    "def get_training_and_validation(dev_audio_path, dev_RTM_path):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
