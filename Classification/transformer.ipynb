{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "In this part of the notebook, we will preprocess the data to make it ready for training. We will do the following steps:\n",
    "1. Visualize the RTM data\n",
    "2. Check the sampling rate of the data and resample to consist to 16kHz to enssure uniformity across the dataset\n",
    "3. Extract MFCC features from the audio data and allign with the RTM data to create the final dataset\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the RTM data\n",
    "\n",
    "# All imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as librosa\n",
    "import sys\n",
    "import tqdm as tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Parse the dummy RTTM file\n",
    "def parse_rttm(file_path):\n",
    "    columns = [\n",
    "        \"Type\",\n",
    "        \"File ID\",\n",
    "        \"Channel ID\",\n",
    "        \"Turn Onset\",\n",
    "        \"Turn Duration\",\n",
    "        \"Orthography Field\",\n",
    "        \"Speaker Type\",\n",
    "        \"Speaker Name\",\n",
    "        \"Confidence Score\",\n",
    "        \"Signal Lookahead Time\",\n",
    "    ]\n",
    "    df = pd.read_csv(file_path, sep=\"\\s+\", names=columns)\n",
    "    return df[[\"Turn Onset\", \"Turn Duration\", \"Speaker Name\"]]\n",
    "\n",
    "\n",
    "# Grabs the important data from the RTM file and creats End Time column\n",
    "def prepare_data(rttm_data):\n",
    "    rttm_data[\"Turn Onset\"] = rttm_data[\"Turn Onset\"].astype(float)\n",
    "    rttm_data[\"Turn Duration\"] = rttm_data[\"Turn Duration\"].astype(float)\n",
    "    rttm_data[\"End Time\"] = rttm_data[\"Turn Onset\"] + rttm_data[\"Turn Duration\"]\n",
    "    return rttm_data\n",
    "\n",
    "\n",
    "# Plots the timeline of the speakers\n",
    "def plot_timeline(data):\n",
    "    \"\"\"\n",
    "    Plots the speaker timeline based on the provided data.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The data containing speaker information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    speakers = data[\"Speaker Name\"].unique()\n",
    "    speaker_indices = {speaker: idx for idx, speaker in enumerate(speakers)}\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        start = row[\"Turn Onset\"]\n",
    "        end = row[\"End Time\"]\n",
    "        speaker = row[\"Speaker Name\"]\n",
    "        ax.plot([start, end], [speaker_indices[speaker]] * 2, linewidth=10)\n",
    "\n",
    "    ax.set_yticks(range(len(speakers)))\n",
    "    ax.set_yticklabels(speakers)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Speakers\")\n",
    "    ax.set_title(\"Speaker Timeline\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def debugger_test():\n",
    "    # Test the RTM data\n",
    "    rttm_data = parse_rttm(\"../Dataset/RTMS/Dev/abjxc.rttm\")\n",
    "    rttm_data = prepare_data(rttm_data)\n",
    "    plot_timeline(rttm_data)\n",
    "\n",
    "\n",
    "def debugger_run_all():\n",
    "    devRTM_path = \"../Dataset/RTMS/Dev/\"\n",
    "    # Visaualize all RTM data in the folder devRTM_path\n",
    "\n",
    "    # Get all the files in the folder\n",
    "    files = os.listdir(devRTM_path)\n",
    "    for file in files:\n",
    "        if file.endswith(\".rttm\"):\n",
    "            file_path = os.path.join(devRTM_path, file)\n",
    "            rttm_data = parse_rttm(file_path)\n",
    "            rttm_data = prepare_data(rttm_data)\n",
    "            plot_timeline(rttm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sampling rate of all audio files, allign mfcc with RTM data such that they can be used for training\n",
    "\n",
    "\n",
    "# Get the MFCC of the audio file using librosa\n",
    "def get_mfcc(file_path, hop_length=220):\n",
    "    \"\"\"\n",
    "    Compute the Mel-frequency cepstral coefficients (MFCC) for an audio file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the audio file.\n",
    "    hop_length (int): Number of samples between successive frames.\n",
    "\n",
    "    Returns:\n",
    "    mfcc (ndarray): The computed MFCC coefficients.\n",
    "    sr (int): The sample rate of the audio file.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path)\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y, sr=sr, n_mfcc=13, hop_length=hop_length\n",
    "    )  # Use consistent hop length\n",
    "    return mfcc, sr\n",
    "\n",
    "\n",
    "# Align the MFCC data with the RTM data -- Speaker segmentation for training\n",
    "def align_mfcc(mfcc_data, sr, rttm_path, hop_length=220):\n",
    "    \"\"\"\n",
    "    Aligns MFCC data with RTTM data using a specified hop length.\n",
    "\n",
    "    Args:\n",
    "        mfcc_data (numpy.ndarray): MFCC data.\n",
    "        sr (int): Sampling rate.\n",
    "        rttm_path (str): Path to the RTTM file.\n",
    "        hop_length (int, optional): Number of samples between successive frames. Defaults to 220.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: Each dictionary contains 'Speaker Name' and 'MFCC Segment'.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If required columns are missing in the DataFrame.\n",
    "    \"\"\"\n",
    "    rttm_data = parse_rttm(rttm_path)\n",
    "    rttm_data = prepare_data(rttm_data)\n",
    "\n",
    "    if \"End Time\" not in rttm_data.columns or \"Turn Onset\" not in rttm_data.columns:\n",
    "        raise KeyError(\"Necessary columns are missing from RTTM data.\")\n",
    "\n",
    "    # Convert RTTM times to frame indices\n",
    "    rttm_data[\"Start Frame\"] = (rttm_data[\"Turn Onset\"] * sr / hop_length).astype(int)\n",
    "    rttm_data[\"End Frame\"] = (rttm_data[\"End Time\"] * sr / hop_length).astype(int)\n",
    "\n",
    "    segments = []\n",
    "    num_frames = mfcc_data.shape[1]\n",
    "    audio_duration = num_frames * hop_length / sr\n",
    "    \n",
    "    for _, row in rttm_data.iterrows():\n",
    "        start_frame = row[\"Start Frame\"]\n",
    "        end_frame = row[\"End Frame\"]\n",
    "\n",
    "        # Clip start_frame and end_frame to valid range\n",
    "        start_frame = max(0, min(start_frame, num_frames - 1))\n",
    "        end_frame = max(0, min(end_frame, num_frames))\n",
    "\n",
    "        if start_frame >= end_frame:\n",
    "            print(\n",
    "                f\"Skipping segment with invalid frame range: Start Frame = {start_frame}, End Frame = {end_frame}\"\n",
    "            )\n",
    "            sys.exit(1)\n",
    "            continue\n",
    "\n",
    "        segment_mfcc = mfcc_data[:, start_frame:end_frame]\n",
    "        segments.append(\n",
    "            {\"Speaker Name\": row[\"Speaker Name\"], \"MFCC Segment\": segment_mfcc}\n",
    "        )\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Test get_mfcc on filepath\n",
    "path_wave = \"../Dataset/Audio/Dev/afjiv.wav\"\n",
    "path_rttm = \"../Dataset/RTMS/Dev/afjiv.rttm\"\n",
    "\n",
    "# Get the MFCC and allign it with the RTM data for training --> Working\n",
    "mfcc_test, sampling_rate = get_mfcc(path_wave)\n",
    "test_rttm_data = parse_rttm(path_rttm)\n",
    "test_rttm_data = prepare_data(test_rttm_data)\n",
    "test_alligned = align_mfcc(mfcc_test,sampling_rate,path_rttm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the entire dataset for training and testing\n",
    "Here we will go through all audio/dev wave files and extract the MFCC features and allign with the RTM data to create the final dataset for training and validation. Pick validation of 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [04:35<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Function to gather and align all data\n",
    "def get_training_and_validation(dev_audio_path, dev_RTM_path):\n",
    "    files = os.listdir(dev_audio_path)\n",
    "    all_data = []\n",
    "\n",
    "    for file in tqdm.tqdm(files):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(dev_audio_path, file)\n",
    "            rttm_path = os.path.join(dev_RTM_path, file.replace(\".wav\", \".rttm\"))\n",
    "            mfcc_data, sr = get_mfcc(file_path)\n",
    "            alligned_data = align_mfcc(mfcc_data, sr, rttm_path)\n",
    "            all_data.extend(alligned_data)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Gather all training data\n",
    "all_training_data = get_training_and_validation(\n",
    "    \"../Dataset/Audio/Dev/\", \"../Dataset/RTMS/Dev/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of loaded data: 8268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spk00'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_in_chunks(data, directory, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Saves the list of segment dictionaries to multiple smaller files.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries containing 'Speaker Name' and 'MFCC Segment'.\n",
    "        directory (str): Path to the output directory.\n",
    "        chunk_size (int, optional): Number of items per chunk. Defaults to 1000.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data[i : i + chunk_size]\n",
    "        chunk_file_path = os.path.join(directory, f\"chunk_{i // chunk_size}.pkl\")\n",
    "        with open(chunk_file_path, \"wb\") as f:\n",
    "            pickle.dump(chunk, f)\n",
    "\n",
    "\n",
    "def load_chunks(directory):\n",
    "    \"\"\"\n",
    "    Loads the data from multiple smaller files.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing chunk files.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: Each dictionary contains 'Speaker Name' and 'MFCC Segment'.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    chunk_files = [\n",
    "        f\n",
    "        for f in os.listdir(directory)\n",
    "        if f.startswith(\"chunk_\") and f.endswith(\".pkl\")\n",
    "    ]\n",
    "    for chunk_file in sorted(chunk_files):\n",
    "        chunk_file_path = os.path.join(directory, chunk_file)\n",
    "        with open(chunk_file_path, \"rb\") as f:\n",
    "            chunk = pickle.load(f)\n",
    "            all_data.extend(chunk)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Store andLoad the data\n",
    "save_in_chunks(all_training_data, \"../Dataset/Audio/Dev_npz\", chunk_size=1000)\n",
    "loaded_data = load_chunks(\"../Dataset/Audio/Dev_npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
