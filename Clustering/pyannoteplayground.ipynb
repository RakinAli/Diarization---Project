{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=\"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\")\n",
    "diarization, embeddings = pipeline(\"../wav-files/4b.wav\", return_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33191782  0.12017356 -0.21278852 -0.0035682  -0.14905319  0.33943492\n",
      "  0.17680457 -0.18478882  0.15768221  0.38152137  0.00086053  0.31897414\n",
      "  0.37410823 -0.24575213  0.01352216  0.07771212  0.09743826  0.16467746\n",
      "  0.06073035  0.00298239 -0.06879069 -0.12528123 -0.05138676  0.14591888\n",
      "  0.42609584 -0.134773    0.19598025 -0.05553991 -0.11395623  0.04224063\n",
      "  0.1574573   0.13589676  0.43032372 -0.02420053 -0.12903498 -0.06768087\n",
      " -0.16334328 -0.2863768   0.31869838  0.05427642 -0.13413924 -0.22965384\n",
      " -0.01967582  0.23800495 -0.04720905  0.14934035  0.0864424  -0.17173754\n",
      " -0.15912783 -0.01271255 -0.01357703 -0.3754244   0.1497519  -0.20626143\n",
      "  0.17231083 -0.02306692  0.06042554 -0.02711732  0.26679975 -0.1847159\n",
      "  0.31595337 -0.20826438  0.06936674  0.02659851 -0.18852392 -0.08363135\n",
      " -0.0649899   0.28550622 -0.04408725 -0.07987702  0.20952877 -0.12178029\n",
      "  0.06191258  0.03758544  0.26111132 -0.17728792  0.38014355 -0.0939346\n",
      "  0.13260815  0.14776796 -0.09878066  0.3915056   0.21860145 -0.07798541\n",
      "  0.09015547  0.12752907  0.02470992 -0.14455448  0.1713903   0.0228223\n",
      "  0.24301293 -0.0526012   0.3108226   0.29066813  0.0667221   0.01946779\n",
      "  0.11801034  0.18459028 -0.15564667  0.31814843 -0.09843219 -0.12370816\n",
      " -0.03234497 -0.13228594 -0.26657996 -0.06063723 -0.15070984 -0.22905654\n",
      " -0.04892685 -0.08197123  0.21807733 -0.18135214 -0.320807   -0.01748921\n",
      "  0.2518825  -0.06000648 -0.07304284 -0.09472665  0.01586522  0.03042869\n",
      " -0.11720187 -0.28457704 -0.12055961 -0.12886165  0.12141708 -0.12528114\n",
      " -0.03587492  0.04448846  0.2549921   0.00925496  0.09046279 -0.13605276\n",
      "  0.06069334 -0.12426404  0.12902807 -0.1928364   0.0807687  -0.16496745\n",
      " -0.35572976  0.0546157   0.08791412 -0.06699279  0.06278087  0.23750463\n",
      " -0.16733313  0.18160114 -0.04415388 -0.3986236   0.2288065   0.04729272\n",
      "  0.07064386 -0.1500624   0.01260221 -0.3159316  -0.10355794  0.05906688\n",
      "  0.16109464 -0.07337627  0.04064035 -0.08882111 -0.49397498  0.27770174\n",
      "  0.12005766  0.11144587  0.0182604   0.16598795 -0.01245098 -0.11998094\n",
      " -0.06039092  0.10771725 -0.11058426  0.04875764  0.12319033  0.07838876\n",
      " -0.03909396 -0.34740904 -0.14210212  0.07921144 -0.3243549   0.15406306\n",
      " -0.0862464  -0.13101006 -0.4141072  -0.14016587 -0.1276232  -0.23038344\n",
      "  0.02277396 -0.08991182  0.06057983 -0.08287567  0.14794819 -0.1826246\n",
      " -0.5163777  -0.0357696  -0.08014591  0.21900228  0.24612485  0.3113017\n",
      "  0.07666397  0.0155732  -0.34210387  0.08431113  0.24881664 -0.06100827\n",
      "  0.08265202  0.17634854 -0.01331476 -0.17868926  0.34938258  0.00232288\n",
      " -0.22153997  0.12701325  0.430863   -0.04756236 -0.16139743 -0.05433707\n",
      " -0.19069742  0.06472649 -0.08636095 -0.06915648  0.17244618 -0.23785855\n",
      "  0.01877035 -0.23098196 -0.05805158 -0.30122012  0.14246677  0.01868804\n",
      "  0.24397415  0.01523983 -0.18068239  0.17656213  0.03527208  0.28875276\n",
      "  0.04255209 -0.00591227 -0.03194764  0.05465542 -0.20338468  0.09344323\n",
      "  0.07440911  0.00689326  0.01551735 -0.19025306  0.5287274   0.10785061\n",
      " -0.26007444  0.29771063 -0.06365558 -0.00606279  0.1746898  -0.04445496\n",
      "  0.02409748  0.08024892  0.1957285   0.10610222] is the embedding of speaker SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "# return embeddings of a speaker in a specific file\n",
    "for s, speaker in enumerate(diarization.labels()): \n",
    "    print(f\"{embeddings[s]} is the embedding of speaker {speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=0.1s stop=0.8s speaker_SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "# return speaker's talking-time and time-stamps\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: either play with pyannote's Clustering module or take your embeddings and do clustering with them directly (if not too complicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "from pyannote.audio.utils.signal import Binarize\n",
    "from pyannote.audio.pipelines.clustering import Clustering\n",
    "from pyannote.core import SlidingWindowFeature\n",
    "from pyannote.metrics.diarization import GreedyDiarizationErrorRate\n",
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedPipeline(Pipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vad = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\", use_auth_token=\"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\")\n",
    "\n",
    "        # load pre-trained speaker embedding model\n",
    "        self.speaker_embedding = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=\"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\")\n",
    "        \n",
    "        # Binarize VAD scores with a 0.5 threshold.\n",
    "        self.binarize = Binarize(offset=0.52, onset=0.52, min_duration_off=0.1, min_duration_on=0.1)\n",
    "\n",
    "        # Diariozation error rate\n",
    "        self.diarization_error_rate = {\"collar\": 0.0, \"skip_overlap\": False}\n",
    "\n",
    "\n",
    "    def apply(self, audio: dict):\n",
    "        # Extract waveform and sample rate from the audio dictionary\n",
    "        print('Extracting waveform and sample rate ...')\n",
    "        uri = audio['uri']\n",
    "        waveform = audio['waveform']\n",
    "        sample_rate = audio['sample_rate']\n",
    "        \n",
    "        # Apply VAD\n",
    "        print('Applying VAD ...')\n",
    "        vad_scores = self.vad({'waveform': waveform, 'sample_rate': sample_rate})\n",
    "\n",
    "        speech = vad_scores\n",
    "\n",
    "        # Apply speech embedding\n",
    "        print('Applying speech embedding ...')\n",
    "        embeddings = self.speaker_embedding(waveform)\n",
    "        print(embeddings)\n",
    "\n",
    "        # Transform embeddings to match VAD segments\n",
    "        print('Transforming embeddings to match VAD segments ...')\n",
    "        #vad_embeddings = embeddings.crop(speech)\n",
    "         # create binary mask from vad_scores\n",
    "        mask = np.zeros_like(embeddings.detach().numpy())\n",
    "        hop_length_seconds = 0.01\n",
    "        for segment in vad_scores.itertracks():\n",
    "            print('Segment 0: ', segment[0])\n",
    "            print('Segment 1: ', segment[1])\n",
    "            print('Segment type: ', type(segment[0]))\n",
    "            start_index = int(segment[0] / hop_length_seconds)\n",
    "            end_index = int(segment[1] / hop_length_seconds)\n",
    "            mask[start_index:end_index] = 1\n",
    "\n",
    "        # use VAD scores to modify embeddings\n",
    "        vad_embeddings = embeddings * mask\n",
    "\n",
    "        # Perform your custom spectral clustering on the embeddings (OUR CUSTOM SPECTRAL CLUSTERING ALGORITHM)\n",
    "        print('Performing custom spectral clustering ...')\n",
    "        clusters = Clustering(vad_embeddings.data)\n",
    "\n",
    "        # Convert the clusters back into pyannote.core.Annotation format and return\n",
    "        return SlidingWindowFeature(clusters, vad_embeddings.sliding_window)\n",
    "    \n",
    "    def get_metric(self) -> GreedyDiarizationErrorRate:\n",
    "        return GreedyDiarizationErrorRate(**self.der_variant)\n",
    "\n",
    "\n",
    "    # our own apply\n",
    "    # def apply(self, audio: dict):\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\josep\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\059e96f964841d40f1a5e755bb7223f76666bba4\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.3.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1a62cbc0b848878ba42488b2aca8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/96.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b3dfc4e89c4673b530a4816718b4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/2.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\josep\\.cache\\torch\\pyannote\\models--pyannote--embedding\\snapshots\\4db4899737a38b2d618bbd74350915aa10293cb2\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.3.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\josep\\.cache\\torch\\pyannote\\models--pyannote--embedding\\snapshots\\4db4899737a38b2d618bbd74350915aa10293cb2\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.3.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "custom_pipeline = CustomizedPipeline()\n",
    "diarization = custom_pipeline(\"../wav-files/4b.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'itertracks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# return speaker's talking-time and time-stamps\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m turn, _, speaker \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdiarization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitertracks\u001b[49m(yield_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mturn\u001b[38;5;241m.\u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms stop=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mturn\u001b[38;5;241m.\u001b[39mend\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms speaker_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'itertracks'"
     ]
    }
   ],
   "source": [
    "# return speaker's talking-time and time-stamps\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
